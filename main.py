import requests, re, concurrent.futures, time

# ================= ä½ çš„ä¸“å±é…ç½® =================
# åŒ…å«ç‰¹æ®Šå­—ç¬¦å…³é”®è¯ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨å¤„ç†ç¼–ç 
KEYWORDS = ["æ¸¯å°", "ç¿¡ç¿ ", "å‡¤å‡°", "ç»å…¸", "é‚µæ°", "ç§å¯†", "ä¸€æœ¬é“", "æ˜Ÿç©º", "ç”µå½±", "ğŸŒ", "ğŸ’"]
SCAN_DEPTH = 500  # æ·±åº¦å¢åŠ åˆ°500ï¼Œç¡®ä¿è¦†ç›–æœ€è¿‘24å°æ—¶
# ===============================================

BASE_URL = "https://ox.html-5.me/i/"
# æ¨¡æ‹Ÿæè‡´çœŸå®çš„æ‰‹æœºè¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Mobile/15E148 Safari/604.1',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-cn',
    'Referer': 'https://ox.html-5.me/'
}
OUT_FILE = "bootstrap.min.css"

def get_latest_id():
    try:
        # å°è¯•ä»é¦–é¡µæŠ“å–æœ€æ–°ç¼–å·
        r = requests.get("https://ox.html-5.me/", timeout=15, headers=HEADERS)
        r.encoding = 'utf-8'
        ids = re.findall(r'/i/(\d+)\.txt', r.text)
        if ids:
            return max(map(int, ids))
    except: pass
    return 8732100 

def scan_url(file_id, pattern):
    url = f"{BASE_URL}{file_id}.txt"
    results = []
    try:
        # åŠ ä¸Šéšæœºå»¶è¿Ÿé˜²æ­¢è¢«å°
        r = requests.get(url, timeout=8, headers=HEADERS)
        # æ ¸å¿ƒï¼šè‡ªåŠ¨è¯†åˆ«å¹¶è½¬æ¢æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦ç¼–ç 
        r.encoding = r.apparent_encoding if r.apparent_encoding else 'utf-8'
        
        content = r.text
        if r.status_code == 200 and "," in content:
            # æš´åŠ›åŒ¹é…ï¼šåªè¦ç¬¦åˆ åå­—,é“¾æ¥ æ ¼å¼çš„å…¨éƒ¨æå–
            lines = re.findall(r"([^,\n\r]+),(http[^\s\n\r]+)", content)
            for name, link in lines:
                name = name.strip()
                # åªè¦åå­—é‡ŒåŒ…å«å…³é”®è¯æˆ–ç‰¹æ®Šç¬¦å·
                if any(kw in name for kw in KEYWORDS):
                    results.append(f"{name},{link.strip()}")
    except: pass
    return results

def main():
    latest_id = get_latest_id()
    start_id = latest_id - SCAN_DEPTH
    print(f"ğŸ›°ï¸ ç›®æ ‡é”å®šï¼šä» {start_id} æ‰«è¡è‡³ {latest_id}")
    
    all_found = []
    # å…³é”®è¯æ­£åˆ™
    pattern = "|".join(KEYWORDS)

    # ç¨å¾®é™ä½å¹¶å‘ï¼Œé˜²æ­¢è¢«ç½‘ç«™é˜²ç«å¢™æ‹‰é»‘
    with concurrent.futures.ThreadPoolExecutor(max_workers=15) as executor:
        futures = [executor.submit(scan_url, i, pattern) for i in range(start_id, latest_id + 1)]
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res: all_found.extend(res)

    with open(OUT_FILE, "w", encoding="utf-8") as f:
        if not all_found:
            f.write("âš ï¸ æ‰«è¡å®Œæ¯•ä½†å…³é”®è¯æœªåŒ¹é…ï¼Œè¯·ç¡®è®¤ç½‘é¡µæ˜¯å¦æœ‰æ›´æ–°,#genre#\n")
            # è°ƒè¯•ä¿¡æ¯ï¼šæŠŠæ‰«åˆ°çš„æœ€åä¸€ä¸ªæ–‡ä»¶çš„ç¬¬ä¸€è¡Œå†™è¿›å»ï¼Œçœ‹çœ‹æŠ“åˆ°å•¥äº†
            f.write(f"è°ƒè¯•ï¼šæœ€åæ‰«æID {latest_id},http://0.0.0.0\n")
        else:
            f.write(f"ğŸ“º æ‰«è¡å®Œæˆ-æ”¯æŒç‰¹æ®Šå­—ç¬¦[å…±{len(all_found)}ä¸ª],#genre#\n")
            unique_list = sorted(list(set(all_found)))
            for line in unique_list:
                f.write(line + "\n")
            
    print(f"ğŸ ä»»åŠ¡ç»“æŸï¼ŒæŠ“å–åˆ° {len(all_found)} æ¡æ•°æ®ã€‚")

if __name__ == "__main__":
    main()
